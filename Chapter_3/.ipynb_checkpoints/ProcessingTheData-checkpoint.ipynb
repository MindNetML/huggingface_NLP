{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86eab9b-80ec-4a64-95c7-567e0568626c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers torch torchvision torchaudio\n",
    "!pip install -q tokenizers==0.13.3\n",
    "!pip install -q bitsandbytes transformers accelerate gradio thread6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb31a41f-35f1-4d8c-8127-deefbab4ad3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e7d4b9ed8a4b629b3e8b19e1493f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b3c48220d54eb9b612e5000232e826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310664764fe440c19c7303116355c146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fd4ecd6bd643b4a320234411967f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9974d4df43b345aca4079c8c155921c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# continuing from the previous chapter, here is how we would train a sequence classifier on one batch in pytorch\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# same as last chapter\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "# sentences\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "\n",
    "batch = tokenizer(sequences, padding = True, truncation = True, return_tensors = \"pt\")\n",
    "\n",
    "# This part is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d145953-6059-4ff6-9faf-bdfe528cb68b",
   "metadata": {},
   "source": [
    "#### The above code trains the model on just 2 sentences. \n",
    "#### As you can tell, this won't do any good as its just 2 sentences\n",
    "#### To get better results, we need to prepare a bigger dataset\n",
    "#### In this next cell, we will use the MRPC dataset, it consists of 5,801 pairs of sentences, with a label indicating whether they are paraphrased or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde5fb5f-c1d6-4747-907f-4c43faa8c685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9326b0b2582e4bfaac3315d80f27651b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6422dba0defc4c31bb42cdaaaa6c1492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7178d065c4f04b88843f2564661bdac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a335dde81ccc41cfb599500adb17a7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a178c872b85647dda7d04c1ab3c64516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b4f57bd34b4c80a3f3513c162ea5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123092b89d20482ba94e790a7ab843cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The huggingface library provides a simple command to download and cache the dataset on the hub\n",
    "\n",
    "# we can download teh MRPC dataset like this:\n",
    "from datasets import load_dataset\n",
    "\n",
    "# note that we use 'glue'\n",
    "# 'glue' is the benchmark that is composed of 10 datasets including the 'mrpc' dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fec3f-fff9-42a4-9002-8b11bcf45ca2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The 'DatasetDict' object above contains the training, validation, and test sets. Each of those contains several columns ->\n",
    "#### -> (sentence1, sentence2, label, and idx) and a variable number of rows(num_rows)\n",
    "#### The command downloads and caches the dataset by default in '~/.cache/huggingface/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b19cef-48d8-41c3-8576-5c92a99570b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can access each pair of sentences in our raw_datasets object by indexing, like in a dictionary\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b94371b-5cbb-4350-8467-d5b46642ecb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Gyorgy Heizler , head of the local disaster unit , said the coach was carrying 38 passengers .', 'sentence2': 'The head of the local disaster unit , Gyorgy Heizler , said the coach driver had failed to heed red stop lights .', 'label': 0, 'idx': 15}\n",
      "\n",
      " {'sentence1': 'He was arrested Friday night at an Alpharetta seafood restaurant while dining with his wife , singer Whitney Houston .', 'sentence2': 'He was arrested again Friday night at an Alpharetta restaurant where he was having dinner with his wife .', 'label': 1, 'idx': 796}\n"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "print(raw_train_dataset[14]) # element 15 of the training set\n",
    "\n",
    "\n",
    "raw_validation_dataset = raw_datasets[\"validation\"]\n",
    "print(\"\\n\",raw_validation_dataset[86]) # element 87 of validation set\n",
    "\n",
    "# Note: the validation is 87 but since its previously split it'll mark a big number (796) since its technically that number if it wasn't split between train, val, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681d267-b4d7-4e99-84c7-964ca7ee5f2c",
   "metadata": {},
   "source": [
    "#### To preprocess the dataset, we need to convert the text to numbers teh model can make sense of.\n",
    "#### This can be done with a tokenizer\n",
    "#### We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4653ebe1-bd53-4069-8158-627ab1e7d5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "\n",
    "# we already tokenized called the model and AutoTokenizer previously so we'll comment that part out\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# checkpoint = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# remember that the ouput of our dataset is a dictionary with sentence1 and sentence2 as keys \n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
    "\n",
    "# the above tokenizes all instances in the dataset of sentence1 and sentence2 (don't print it lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7395c3e-560d-4640-a5c3-f59ac89ddb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can't just pass 2 sequences to the model and get a prediction of whether the sentences are paraphrased or not\n",
    "\n",
    "# Fortunately, the tokenizer can take a pair of sequences and prepare it the way our BERT model expects\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "inputs\n",
    "\n",
    "# notice the input_ids key and the attention_mask key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf695f-6877-468e-a8a2-447eb8be606d",
   "metadata": {},
   "source": [
    "### 'token_type_ids': This is what tells the model which part of the input is the first sentence and which is the second sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d77e335-d8cf-40b3-9b17-f39ecb71b26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "{'input_ids': [101, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "{'input_ids': [[101, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 2056, 1996, 2873, 2001, 4755, 4229, 5467, 1012, 102], [101, 1996, 2132, 1997, 1996, 2334, 7071, 3131, 1010, 1043, 7677, 22637, 2002, 10993, 3917, 1010, 2056, 1996, 2873, 4062, 2018, 3478, 2000, 18235, 2094, 2417, 2644, 4597, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "ex_element = raw_train_dataset[14]\n",
    "\n",
    "exsent1 = tokenizer(ex_element['sentence1'])\n",
    "exsent2 = tokenizer(ex_element['sentence2'])\n",
    "\n",
    "ex_both = tokenizer([ex_element['sentence1'],ex_element[ 'sentence2']])\n",
    "\n",
    "print(exsent1)\n",
    "print(\"\\n\")\n",
    "print(exsent2)\n",
    "print(\"\\n\")\n",
    "print(ex_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "204469b3-d26f-4866-a4a7-c0b4a4417965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'sentence',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'one',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can decode the IDs inside input_ids back to words\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f868d6-8589-4fa2-9f84-f2926a35e87d",
   "metadata": {},
   "source": [
    "#### The parts of the above input correspond to [CLS] sentence1 [SEP] all have a token type ID of 0.\n",
    "#### While the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1.\n",
    "\n",
    "#### Note that if you select a different checkpoint, you'll probably have different token_type_ids in your tokenized inputs.\n",
    "   * For example, they're not returned if you used a DistilBERT model\n",
    "\n",
    "#### They are only returned when the model knows what to do with them\n",
    "   * This is because the model has seen them during its pretraining\n",
    "\n",
    "#### The model were using above is BERT, it has 2 objectives(things its trained to do)\n",
    "   * Masked language modeling (predicting a missing word)\n",
    "   * Next sentence predicition (Model the relationship between pairs of sentences)\n",
    "      * This is done by providing MASKED sentences to the model in pairs, with half being from the same paragraph and the rest are random. \n",
    "      * The model is then tasked with predicting if the second sentence follows the first\n",
    "      \n",
    "#### You don't need to worry about token_type_ids in your tokenized input\n",
    "   * This is as long as you use the same checkpoint for the tokenizer and the model\n",
    "   * Everything will be fine as the tokenizer knows what to provide to its model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f3eb459-8070-4585-8b67-a4575c69003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can tokenize the whole dataset\n",
    "\n",
    "# we'll feed the tokenizer a list of pairs of sentences, we can include 'padding' and 'truncation'\n",
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding = True,\n",
    "    truncation = True,\n",
    ")\n",
    "# This works well but has the disadvantage of returning a dictionary\n",
    "# Another disadvantage is that it will attempt to store the whole dataset in the RAM during tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45478e03-9bc4-40af-bf83-da4303f4110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep the data as a dataset, we'll use the 'Dataset.map' method\n",
    "\n",
    "# This provides more flexibility if we need more preprocessing done other than tokenization\n",
    "\n",
    "# The map method works by applying a function on each element of the dataset, so let's define a function that tokenizes our input\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c509e0-9887-4780-8819-8b14757fe9fe",
   "metadata": {},
   "source": [
    "#### The funciton above takes a dictionary and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids.\n",
    "#### It also works if the example dictionary contains several samples since the tokenizer works on lists of pairs of sentences\n",
    "#### This alows us to use the option 'batched = True' in our call to 'map()', which will greatly speed up tokenization\n",
    "\n",
    "#### We've left the padding argument out of our function for now. This is because padding all the samples to the max length is not efficient\n",
    "   * Its better to pad the samples when we're building a batch, as then we only need to pad to the maximum length in that batch, and not the maximum length of the dataset\n",
    "       * This can save time and processing power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b37c9cb-3bbe-4604-b8f2-7ce4c215fc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f54d518c8f4c4e8398ee016551e8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e5f4a85bfa438aa6345c2f497f4aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ee2165d5fb4da29fac9bcf515f3cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how we apply the tokenization function to all our datasets at once.\n",
    "# we're using 'batched=True' in our call to 'map()' so that the function is applied to multiple elements of our dataset at once and not seperatly.\n",
    "# This allows for faster processing\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched = True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893395f8-14f3-4ccd-89ff-df7638c9149a",
   "metadata": {},
   "source": [
    "#### The function that is reponsible for putting together samples inside a batch is called the 'collate function'\n",
    "#### It's an argument you can pass when building a 'Data Loader'\n",
    "   * The default being a function that will just convert your samples to PyTorch tensors and concatenate them\n",
    "   \n",
    "#### We have deliberately postponed the padding, to only apply it as necessary on each batch and avoid having over-long inputs with a lot of padding\n",
    "   * Example: a input_id that has like 3 digits and 20 0's since it had a few words in the original sentence but the longest sentence in the batch was 23 tokens long\n",
    "  \n",
    "#### To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together.\n",
    "   * The 🤗 Transformers library provides such a function via 'DataCollatorWithPadding' and will do everything we need\n",
    "       * It takes a tokenizer when you instantiate it(to know which padding token to use, and whether the model expects padding to be on the left or on the rightof the inputs)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83457e52-617e-4ed3-b2dd-efff4221dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7806d7f-9aba-416f-9976-33313f876a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test it by grabbing a few samples of our training set that we would like to batch together\n",
    "\n",
    "# We remove idx, sentence1, and sentence2 as they won't be needed\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in ['idx', 'sentence1', 'sentence2']}\n",
    "\n",
    "[len(x) for x in samples['input_ids']]\n",
    "\n",
    "# our output was between 32-67, dynmaic paddin means all samples in the batch should all be padded to a length of 67, the max length inside the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94d29811-f608-4603-aba8-79766bce6186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's double check that our 'data_collator' is dynamically padding the batch properly\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bb77bc9-7a3a-4d95-9451-7ed0d80bc65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209b1783abf5483aa91a64d99abda6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731529cd0ae345d9a0e68a93ea25c2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise\n",
    "\n",
    "# 'glue' is the benchmark that is composed of 10 datasets including the 'sst-2' dataset\n",
    "sst_raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "sst_raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48d07d-61f8-4377-b20a-8a07be67bccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
