{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9eb0f83-84c0-49f0-bbba-079c31ee3d7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers torch torchvision torchaudio\n",
    "!pip install -q tokenizers==0.13.3 evaluate\n",
    "!pip install -q bitsandbytes transformers accelerate gradio thread6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abaaea7f-a536-4ddc-b7bd-f5093b3f5a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cd7986eabb4220893375280c476699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0813fa853a4b1b95613a45bb9d2112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa6638ef77e4094ada16c1a6cd3a33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9cf4fe28b84cd6a63130dcb5e55e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf7e4f724ed45d19b5c6b2749a92f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f6ad0eb0794e0bafe7daf0f968988c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9d1b8b64b34864b7047cb39690ba16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409d4dec97084771bcbeba37970af532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe04fdfe71a406fb8ab63a4959cb665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cace14f0bc4f1e8fb2461a084a6a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74e367576b447628662a0ad639b6d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f6e90075114246995f1d8550c9fd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f097c1bd166d42f38695cbf642063086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6702324dab343b4a3d0858c3d9ec280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Everything we've done in the last few chapters\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation = True)\n",
    "\n",
    "# The primary purpose of the 'map()' function is to apply a given function (in this case, 'tokenize_function') to each element in the dataset.\n",
    "# The 'batched=True' argument, indicates that the 'tokenize_function' should be applied to batches of examples rather that individual examples. This can be more efficient, especially when tokenizing text ->\n",
    "# -> because many tokenizers (including huggingfaces) can process multiple sequences at once more quickly than processing each sequence individually.\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched = True)\n",
    "\n",
    "# 'data_collator' is a function or collable that processes a batch of data and prepares it for input into a model during training or evaluation.\n",
    "# In this context the 'DataCollatorWithPadding' is a specific type of data collator designedto handle tokenized text data\n",
    "# It pads sequences in each batch to the length of the longest sequence in that batch. Shorter sequences are padded with the appropriate padding token to ensure consistent input sizes.\n",
    "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143fb6c-989a-4cd3-a39f-9395ff8a3cd0",
   "metadata": {},
   "source": [
    "### Truncation: The purpose of truncation is to ensure that sequences do not exceed a certain length, typically the maximum length that a model can handle. If a sequence is longer than this maximum, it gets cut off (or truncated) to fit. This is especially important for models like BERT, which have a fixed maximum input size (e.g., 512 tokens).\n",
    "### Padding: Padding deals with sequences that are shorter than the maximum length or the longest sequence in a batch. In order to process a batch of sequences simultaneously, all sequences in that batch need to have the same length. Padding adds extra tokens (usually zeros or a special padding token) to the end of shorter sequences to ensure that all sequences in the batch have the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe250ae-22bc-44eb-a368-8d363a8686d0",
   "metadata": {},
   "source": [
    "#### Before we write our training loop, we will need to define a few objects.\n",
    "#### The first ones will be the dataloaders we will use to iterate over batches.\n",
    "   * Before we do this we'll need to apply a bit of postprocessing to our tokenized_datasets to take care of some things that the 'Trainer' did for us automatically, like:\n",
    "       * Remove columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns)\n",
    "       * Rename the column 'label' to 'labels' (because the model expects the argument to be named 'labels')\n",
    "       * set the format of the datasets so they return PyTorch tensors instead of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61178267-3426-423b-a1f4-d1584de90328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our 'tokenized_datasets' has one method for each of those steps\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd59f7-e6eb-4232-a242-c69664694a91",
   "metadata": {},
   "source": [
    "## Post-processing\n",
    "   * Reasoning behind steps in the above code\n",
    "   \n",
    "### Tokenized Versions Replace Original Text\n",
    "   * Once you've tokenized sentence1 and sentence2, you've essentially converted them into a format that the model can understand and process (i.e., sequences of token IDs). These tokenized versions are what the model will actually use during training. The original text columns (sentence1 and sentence2) are no longer needed because the model doesn't operate directly on raw text.\n",
    "   \n",
    "* While sentence1 and sentence2 was the data we initially used, by the time we're ready to fine-tune, they've been transformed into a format suitable for the model (token IDs). The original text columns are then superfluous and can be safely dropped to streamline the dataset.\n",
    "\n",
    "####  The original sentence1 and sentence2 text data are transformed into several new columns that are suitable for input into transformer models.\n",
    "   * input_ids: This column contains the tokenized version of your text. Each text sequence is converted into a sequence of token IDs based on the tokenizer's vocabulary. This is the primary input to the model.\n",
    "   * token_type_ids: For models like BERT that can handle pair-wise sentence tasks (e.g., question-answering, sentence-pair classification), this column indicates which tokens belong to sentence1 and which belong to sentence2. Typically, tokens from sentence1 might be marked with 0 and tokens from sentence2 with 1. This helps the model distinguish between the two sentences when they are concatenated together.\n",
    "   * attention_mask: This column indicates which tokens are actual content versus which ones are padding. A value of 1 typically indicates a real token, while a value of 0 indicates a padding token. The attention mask ensures that the model doesn't pay attention to padding tokens during training or inference.\n",
    "   * labels: This column contains the labels for your training data, which the model will use as the \"ground truth\" during supervised training. It was renamed from label to labels in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7a27e-9c7a-40e1-9b95-af578827b5c7",
   "metadata": {},
   "source": [
    "### Now that the above is done, we can define our \"DataLoaders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56082f2b-6095-4f1d-b26e-43be1baf2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Training dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle = True, batch_size = 8, collate_fn = data_collator\n",
    ")\n",
    "\n",
    "# Evaluation dataloader\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size = 8, collate_fn = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579cbedd-6ea1-403e-9523-9518acd34427",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "\n",
    "- A DataLoader is a utility provided by PyTorch in the torch.utils.data module. It's used to efficiently load and batch your data during training and evaluation. Here's why they're important:\n",
    "    - **Batching**: Neural networks are typically trained using batches of data rather than one sample at a time. Batching allows for more efficient and parallelized processing, especially on GPUs. DataLoader automates the process of fetching batches of data.\n",
    "    - **Shuffling**: For training, it's often beneficial to shuffle the data to ensure that the model doesn't learn any unintended patterns from the order of the data. Shuffling can help improve model generalization.\n",
    "    - **Parallel Loading**: DataLoader can use multiple worker processes to load data in parallel, which can significantly speed up data loading, especially when the data loading process is I/O bound.\n",
    "    - **Memory Efficiency**: Instead of loading the entire dataset into memory, DataLoader loads data on-the-fly in batches, which is more memory-efficient, especially for large datasets.\n",
    "    \n",
    "### The Above code\n",
    "\n",
    "```python\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "```\n",
    "* **tokenized_datasets[\"train\"]**: This is the training portion of your tokenized dataset.\n",
    "* **shuffle=True**: This ensures that the training data is shuffled before batching.\n",
    "* **batch_size=8**: This specifies that each batch should contain 8 samples.\n",
    "* **collate_fn=data_collator**: The collate_fn is a function that takes a list of samples and merges them into a batch. Here, you're using the data_collator you defined earlier (which handles padding) as the collate function.\n",
    "\n",
    "### How the DataLoaders interacts with our data\n",
    "- The tokenized_datasets you created earlier contains the tokenized versions of your data, ready to be fed into a model. However, to efficiently feed this data into a model during training and evaluation, you need to batch it, possibly shuffle it, and handle any last-minute processing (like padding). That's where the DataLoader comes in.\n",
    "- The DataLoader will fetch batches of data from tokenized_datasets, and for each batch, it will use the data_collator to ensure that the sequences in the batch are padded appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5ca7d-3ad0-4d46-b590-d4ae29de778c",
   "metadata": {},
   "source": [
    "### To quickly check for any mistakes in the data processing, we can inspect a batch like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63c4d2f-7523-4788-8790-38705e18f119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([8]),\n",
       " 'input_ids': torch.Size([8, 72]),\n",
       " 'token_type_ids': torch.Size([8, 72]),\n",
       " 'attention_mask': torch.Size([8, 72])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\n",
    "\n",
    "# Note: the actual shape will differ since we set 'shuffle=True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82191314-737d-44ca-b3e0-249665556cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7226d224b61e4d60827aaac02434dbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We are now finished with the data preprocessing step\n",
    "# Now we instantiate the model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38aac667-b932-42f4-bdfc-17d17c134d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7956, grad_fn=<NllLossBackward0>) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "# To make sure everything will go smoothly during training, we pass our batch to this model\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe2d9f-1e0d-4490-b398-bbd5c71d3048",
   "metadata": {},
   "source": [
    "### The above code\n",
    " - This code snippet is essentially a \"sanity check\" to ensure that everything is set up correctly before proceeding to full-scale training. By forwarding a batch through the model and examining the outputs:\n",
    "```python\n",
    "outputs = model(**batch)\n",
    "print(outputs.loss, outputs.logits.shape)\n",
    "```\n",
    "- **model(*\\*batch)**:\n",
    "    * This line is forwarding a batch of data through the model. The **batch syntax is Python's way of unpacking a dictionary into keyword arguments. In this context, the batch is likely a dictionary containing the tokenized inputs for the model, such as ***input_ids, attention_mask, and possibly token_type_ids and labels.***\n",
    "    * The model, which is an instance of AutoModelForSequenceClassification, is designed for tasks like sentence pair classification. When you forward data through it, it returns a structure containing various outputs.\n",
    "\n",
    "- **outputs.loss**:\n",
    "    * This is the model's computed loss for the given batch. If the batch contains labels, the model will compute the loss by comparing its predictions (logits) to the ground truth labels. This loss is what you'd typically optimize during training using an optimizer.\n",
    "    \n",
    "- **outputs.logits.shape**:\n",
    "    * logits are the raw, unnormalized scores output by the model for each class. For a binary classification task (since num_labels=2), there will be two logits for each input example: one for each class.\n",
    "    * By printing the shape of the logits, you're likely checking the dimensions of the output to ensure they match your expectations. For a batch size of 8 and 2 classes, you'd expect the shape to be [8, 2].\n",
    "    \n",
    "### Output Explanation:\n",
    "\n",
    "*tensor(0.7956, grad_fn=<NllLossBackward0>) torch.Size([8, 2])*\n",
    "* **tensor(0.7956, grad_fn=<NllLossBackward0>)**: This is the loss value for the batch. It's a single scalar value, and as mentioned, the grad_fn part indicates that this tensor is ready for gradient computations during backpropagation.\n",
    "* **torch.Size([8, 2])**: This indicates that the logits tensor has a shape of 8x2. As explained, this means you have 8 samples in the batch and 2 scores (logits) for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6464004-a043-464d-9987-043e8fbc98a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# were almost ready to write our training loop! We're just missing the optimizer and the learning rate schedular.\n",
    "# Since we are trying to replicate what the 'Trainer' was doing by hand, we will use the same defaults.\n",
    "# The optimizer used by the Trainer is 'AdamW', which is the same as Adam, but with a twist for weight decay regularization.\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f414f5d-88e7-438c-993b-27ee0165020c",
   "metadata": {},
   "source": [
    "### Weight Decay Regularization\n",
    "- Weight decay is a form of regularization used in neural network training. Regularization is a technique to prevent overfitting by adding some form of penalty to the loss function. Overfitting occurs when a model performs very well on the training data but poorly on unseen data, indicating that it has become too complex and has started to memorize the training data rather than generalizing from it.\n",
    "\n",
    "- The idea behind weight decay is to add a penalty to the loss that is proportional to the size of the model's weights. By doing this, the training process is discouraged from setting large values to the weights, leading to a simpler and more regularized model.\n",
    "\n",
    "$$\n",
    "L_{\\text{new}} = L + \\frac{\\lambda}{2} \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "\n",
    "* $L$ - is the original loss.\n",
    "\n",
    "* $λ$ - is the weight decay coefficient, determining the strength of the regularization.\n",
    "\n",
    "* $w_i$ - are the model's weights.\n",
    "\n",
    "#### The AdamW optimizer is a variant of the Adam optimizer that correctly implements weight decay regularization, as opposed to the \"decoupled weight decay\" in the original Adam. This makes AdamW particularly suitable for tasks where weight decay has been found beneficial, such as fine-tuning transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9e5e169-e5f2-438f-a58b-a4abe67a758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1377\n"
     ]
    }
   ],
   "source": [
    "# Finally, the learning rate schedular used by default is just 'linear decay' from the maximum value (5e-5) to 0. To properly define it, we need to know the number of training steps we will take.\n",
    "# This is the number of 'Epochs' we want to run multiplied by the number of training batches (which is length of our training dataloader) The 'Trainer' uses 3 epochs by default so we'll use it as well\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# An \"epoch\" refers to one complete pass through the entire training dataset. If you set num_epochs to 3, it means you intend to train the model on the entire training dataset three times.\n",
    "num_epochs = 3\n",
    "\n",
    "# By multiplying the number of epochs by the number of batches, you get the total number of training steps. A \"training step\" refers to a single update of the model's weights, which happens once per batch.\n",
    "num_training_steps = num_epochs * len(train_dataloader) # This gives you the number of batches in your training dataset. If you have, for example, 1000 training samples and a batch size of 8, you'd have 125 batches.\n",
    "\n",
    "# The learning rate scheduler adjusts the learning rate during training\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", # The \"linear\" scheduler gradually decreases the learning rate from the initial value to zero.\n",
    "    optimizer = optimizer, # The optimizer 'AdamW' made this in the last code cell\n",
    "    num_warmup_steps = 0, # This specifies how many steps to linearly increase the learning rate before starting to decrease it. Setting it to 0 means there's no warm-up phase, and the learning rate will start decreasing from the beginning.\n",
    "    num_training_steps = num_training_steps, # This is the total number of training steps you calculated earlier. It tells the scheduler over how many steps to decrease the learning rate to zero.\n",
    ")\n",
    "\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8feb3-8aa6-473f-8b67-561a0976c080",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "- one last thing, we want to use the GPU if have access to one(CPU will take hours-days vs a GPU minutes-hours, depending on data) \n",
    "- To do this we define a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e608e511-491e-43fc-aa15-4881bdd83685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9daf0ba-7ced-4805-b0f9-8b208723bdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ead1b285c36498b863bb4867d607f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We will add a progress bar over our number of training steps, using the 'tqdm' library\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "# We didn't put any reporting so this training loop will not tell us anything about how the model fares. We need an evaluation loop for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d7c5130-d70f-472b-95e4-bd67934fdcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7226a55a8d47a985a8599607cb891d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7769607843137255, 'f1': 0.8460236886632826}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We've already seen the 'metric.compute' method, but the metrics can accumulate batches for us as we go over the prediction loop with the 'add_batch' method\n",
    "# Once we have accumulated all the batches, we can get the final result with 'metric.compute()'\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim = -1)\n",
    "    metric.add_batch(predictions = predictions, references = batch['labels'])\n",
    "    \n",
    "metric.compute()\n",
    "# results will vary slightly because of randomness in the model head initilization and the data shuffling but they should be in the same ballpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ed8d2c-aad9-4e90-a79f-07eea92ed382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise\n",
    "\n",
    "# Modify the previous training loop to fine-tune your model on the SST-2 dataset.\n",
    "\n",
    "# I'll come back to it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e68c6f-b9b4-418c-9858-5248a0208230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly we talked about the 'Accelerate' library which can enable dstribution on multiple GPU's or TPU's\n",
    "# Here's the code to run it\n",
    "# Note: Don't run\n",
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dl:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96c442-e093-4685-9712-71ff3e2ac3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above is to be made into a 'train.py' file\n",
    "# to run the 'train.py' simply enter the command 'accelerate config' on a terminal\n",
    "# it'll prompt you to answer a few questions and dump your answers in a configuration file used by this command 'accelerate launch train.py'\n",
    "\n",
    "# If you want to run it in a notebook like this one, use this: \n",
    "# Note: Don't run\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
