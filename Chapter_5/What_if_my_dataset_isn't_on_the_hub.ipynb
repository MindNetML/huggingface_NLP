{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1010c692-6f06-4bce-af0b-82c455ceff6f",
   "metadata": {},
   "source": [
    "### In this section we'll see how the ðŸ¤— Datasets library can be used to load datasets that arn't available on the hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b275e9b-6b13-45a6-9de7-dc242bc0f92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install git-lfs (required for using git)\n",
    "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "!sudo apt-get install git-lfs\n",
    "\n",
    "# necessary packages to use huggingface with paperspace\n",
    "!pip install -q --upgrade transformers torch torchvision torchaudio\n",
    "!pip install -q tokenizers==0.13.3 evaluate\n",
    "!pip install -q bitsandbytes transformers accelerate gradio thread6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d44eca-6851-4ec8-8803-4885bbbcd010",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Working with local and remote datasets\n",
    "\n",
    "- ðŸ¤— Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ceda96-3b2d-4043-bd1e-8fe0b100c53f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-3.9.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prettytable) (0.2.6)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# we'll use the 'prettytables' package for formatting our data\n",
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5b970c0-062c-458a-b6a8-934dac1d61d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        table {\n",
       "            border-collapse: collapse;\n",
       "            width: 46%;\n",
       "            margin: 20px 0;\n",
       "            font-size: 16px;\n",
       "        }\n",
       "        th, td {\n",
       "            padding: 10px !important;\n",
       "            border-bottom: 1px solid #ddd;\n",
       "            background-color: #f2f2f2;\n",
       "            text-align: center !important;\n",
       "        }\n",
       "        /* Center-align the column labels */\n",
       "        th {\n",
       "            background-color: #4CAF50;\n",
       "            color: white;\n",
       "            text-align: center !important;\n",
       "        }\n",
       "        tr:hover {\n",
       "            background-color: #e0e0e0;\n",
       "        }\n",
       "        th {\n",
       "            background-color: #4CAF50;\n",
       "            color: white;\n",
       "        }\n",
       "        /* Add these styles to adjust column widths */\n",
       "        td:nth-child(1), th:nth-child(1) {\n",
       "            width: 20%;  /* Adjust as needed */\n",
       "        }\n",
       "        td:nth-child(2), th:nth-child(2) {\n",
       "            width: 10%;  /* Adjust as needed */\n",
       "        }\n",
       "    </style>\n",
       "    <table>\n",
       "    <tr><th><h4>Data format</h4></th><th><h4>Loading script</h4></th><th><h4>Example</h4></th></tr><tr><td><h4>CSV & TSV</h4></td><td><h4>csv</h4></td><td><h4>load_dataset(\"csv\", data_files=\"my_file.csv\")</h4></td></tr><tr><td><h4>Text files</h4></td><td><h4>text</h4></td><td><h4>load_dataset(\"text\", data_files=\"my_file.txt\")</h4></td></tr><tr><td><h4>JSON & JSON Lines</h4></td><td><h4>json</h4></td><td><h4>load_dataset(\"json\", data_files=\"my_file.jsonl\")</h4></td></tr><tr><td><h4>Pickled DataFrames</h4></td><td><h4>pandas</h4></td><td><h4>load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")</h4></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_table(data):\n",
    "    # Add some basic CSS for styling\n",
    "    html = \"\"\"\n",
    "    <style>\n",
    "        table {\n",
    "            border-collapse: collapse;\n",
    "            width: 46%;\n",
    "            margin: 20px 0;\n",
    "            font-size: 16px;\n",
    "        }\n",
    "        th, td {\n",
    "            padding: 10px !important;\n",
    "            border-bottom: 1px solid #ddd;\n",
    "            background-color: #f2f2f2;\n",
    "            text-align: center !important;\n",
    "        }\n",
    "        /* Center-align the column labels */\n",
    "        th {\n",
    "            background-color: #4CAF50;\n",
    "            color: white;\n",
    "            text-align: center !important;\n",
    "        }\n",
    "        tr:hover {\n",
    "            background-color: #e0e0e0;\n",
    "        }\n",
    "        th {\n",
    "            background-color: #4CAF50;\n",
    "            color: white;\n",
    "        }\n",
    "        /* Add these styles to adjust column widths */\n",
    "        td:nth-child(1), th:nth-child(1) {\n",
    "            width: 20%;  /* Adjust as needed */\n",
    "        }\n",
    "        td:nth-child(2), th:nth-child(2) {\n",
    "            width: 10%;  /* Adjust as needed */\n",
    "        }\n",
    "    </style>\n",
    "    <table>\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, row in enumerate(data):\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            if i == 0:\n",
    "                html += \"<th><h4>%s</h4></th>\" % field\n",
    "            else:\n",
    "                html += \"<td><h4>%s</h4></td>\" % field\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    \n",
    "    display(HTML(html))\n",
    "\n",
    "# Define your table data\n",
    "data = [\n",
    "    [\"Data format\", \"Loading script\", \"Example\"],\n",
    "    [\"CSV & TSV\", \"csv\", 'load_dataset(\"csv\", data_files=\"my_file.csv\")'],\n",
    "    [\"Text files\", \"text\", 'load_dataset(\"text\", data_files=\"my_file.txt\")'],\n",
    "    [\"JSON & JSON Lines\", \"json\", 'load_dataset(\"json\", data_files=\"my_file.jsonl\")'],\n",
    "    [\"Pickled DataFrames\", \"pandas\", 'load_dataset(\"pandas\", data_files=\"my_dataframe.pkl\")']\n",
    "]\n",
    "\n",
    "# Display the table using the function\n",
    "display_table(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e385a63-095f-4b0d-ac09-fec35766f67a",
   "metadata": {},
   "source": [
    "- As shown in the table, for each data format we just need to specify the type of loading script in the 'load_dataset()' funciton, along with a 'data_files' argument that specifies the path to one or more files.\n",
    "\n",
    "- Let's start by loading a dataset from local files; later we'll see how to do the same with remote files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176f93c-addd-43b5-b891-202d73dc3bcc",
   "metadata": {},
   "source": [
    "### Loading a local dataset\n",
    "\n",
    "- For this example we'll use the [SQuAD-it](url=\"https://github.com/crux82/squad-it/\") dataset, which is a large-scale dataset for question answering in Italian\n",
    "\n",
    "- The training and test splits are hosted on GitHub, so we can download them with a simple 'wget' command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "295270e8-1627-4f67-b490-d0e1811d6f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/NLP_huggingface/Chapter_5/data\n",
      "--2023-09-15 20:53:03--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz [following]\n",
      "--2023-09-15 20:53:03--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-train.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7725286 (7.4M) [application/octet-stream]\n",
      "Saving to: â€˜SQuAD_it-train.json.gzâ€™\n",
      "\n",
      "SQuAD_it-train.json 100%[===================>]   7.37M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-09-15 20:53:03 (295 MB/s) - â€˜SQuAD_it-train.json.gzâ€™ saved [7725286/7725286]\n",
      "\n",
      "--2023-09-15 20:53:04--  https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz [following]\n",
      "--2023-09-15 20:53:04--  https://raw.githubusercontent.com/crux82/squad-it/master/SQuAD_it-test.json.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1051245 (1.0M) [application/octet-stream]\n",
      "Saving to: â€˜SQuAD_it-test.json.gzâ€™\n",
      "\n",
      "SQuAD_it-test.json. 100%[===================>]   1.00M  --.-KB/s    in 0.006s  \n",
      "\n",
      "2023-09-15 20:53:04 (172 MB/s) - â€˜SQuAD_it-test.json.gzâ€™ saved [1051245/1051245]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the 'data' directory if it doesn't already exist\n",
    "!mkdir -p data\n",
    "\n",
    "# Navigate into the 'data' directory\n",
    "# the '%' makes it persistant when using jupyter notebooks\n",
    "%cd data\n",
    "\n",
    "# This will download two compressed files call 'SQuAD_it-train.json.gz' and 'SQuAD_it-test.json.gz' ->\n",
    "# -> which we can decompress with the Linux gzip command\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz\n",
    "!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aab5432c-4c78-4763-9054-bf6a0fc80f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD_it-test.json.gz:\t 87.5% -- created SQuAD_it-test.json\n",
      "SQuAD_it-train.json.gz:\t 82.3% -- created SQuAD_it-train.json\n"
     ]
    }
   ],
   "source": [
    "# decompresses the 'json.gz' files\n",
    "!gzip -dkv SQuAD_it-*.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e46943-f795-429b-acb1-c2ee232cde85",
   "metadata": {},
   "source": [
    "- We can see that the compressed files have been replaced with 'SQuAD_it-train.json', and 'SQuAD_it-test.json', and that the data is stored in the JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53b82ebb-3b2e-4126-8973-906bbc17beab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 58M\n",
      "-rw-r--r-- 1 root root 8.0M Sep 15 20:53 SQuAD_it-test.json\n",
      "-rw-r--r-- 1 root root 1.1M Sep 15 20:53 SQuAD_it-test.json.gz\n",
      "-rw-r--r-- 1 root root  42M Sep 15 20:53 SQuAD_it-train.json\n",
      "-rw-r--r-- 1 root root 7.4M Sep 15 20:53 SQuAD_it-train.json.gz\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76fd43d-0e6a-4e51-b650-a9053175031f",
   "metadata": {},
   "source": [
    "##### To load a Json file with the 'load_dataset()' function, we just need to know if we're dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines(line-seperated JSON)\n",
    "- Like many questions answering datasets, 'SQuAD-it' uses the nested format, with all the text stored in a data field . This means we can load the dataset by specifying the field argument as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfb6fba3-8be3-455e-868c-d55ec7c38209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3654a07a1266403d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-3654a07a1266403d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51180a978c56476d85a14aa5c7635231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46d88aab67549fc856d4f46099ddf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-3654a07a1266403d/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7f953f51d447d388fb8d3f834094b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad_it_dataset = load_dataset('json', data_files = \"SQuAD_it-train.json\", field = \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbedb2c-ccf6-44de-88b0-b62520bbd9be",
   "metadata": {},
   "source": [
    "- By default, loading local files creates a DatasetDict object with a train split. We can see this by inspecting the squad_it_dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f7ca3ab-e45c-48f8-a40b-f847035ba9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e8f13-9a00-488e-821d-ce6d56b6d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_it_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646e731b-dae2-4646-b046-fbab81179343",
   "metadata": {},
   "source": [
    "- We've loaded our first local dataset!\n",
    "- But while this worked for the training set, what we really want is to include both the train and test splits in a single 'DatasetDict' object so we can apply 'Dataset.map() functions across both splits at once\n",
    "- To do this, we can provide a dictionary to the data_files argument that maps each split name to a file associated with that split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52df2ae4-3f1a-40f7-aecc-f14e1c452045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ebe61a3d1552e88f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ebe61a3d1552e88f/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e36acf501c436ab80a1ead49daae40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419b83c937aa4772a579dade1d214e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ebe61a3d1552e88f/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b76e7ff598e4f27bd271eafb34de724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging both datasets into one\n",
    "data_files = {'train': 'SQuAD_it-train.json', 'test': 'SQuAD_it-test.json'}\n",
    "squad_it_dataset = load_dataset('json', data_files = data_files, field = 'data')\n",
    "squad_it_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c80bf8-ca48-4b11-a8a5-2cf90c184d42",
   "metadata": {},
   "source": [
    "- This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.\n",
    "\n",
    "- The data_files argument of the load_dataset() funcition is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths.\n",
    "\n",
    "- You can also glob files that match a specified pattern according to the rules used by the Unix shell(you can glob all the JSON files in a directory as a single split by setting 'data_files = \"*.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a8681-1c64-4861-baa2-160c643fe7c5",
   "metadata": {},
   "source": [
    "- The loading script in ðŸ¤— Datasets actually support automatic decompression of the input files, so we could have skipped the use of gzip by pointing the 'data_files' argument directly to the compressed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b64642bd-bc84-4d64-9ee6-682259338473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-49a878f2f428dbbf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-49a878f2f428dbbf/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5af5bcbd3d4d33aea8f633330ad751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a829b708296b4a42ad3bc432273b8d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-49a878f2f428dbbf/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa73025b1f1427984e8ee66a2beffea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\"train\": \"SQuAD_it-train.json.gz\", \"test\": \"SQuAD_it-test.json.gz\"}\n",
    "squad_it_dataset = load_dataset(\"json\", data_files = data_files, field = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aca887ed-fc9d-4493-852e-6e54c79f8831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 442\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['title', 'paragraphs'],\n",
       "        num_rows: 48\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_it_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed912e55-3988-419e-a571-2600b8523ac3",
   "metadata": {},
   "source": [
    "1. Now you don't manually have to decompress gzip files anymore!\n",
    "    - The decompression applies to other common formats such as '.zip' and '.Tar'\n",
    "    - you just need to point 'data_files' to the compressed files and you're good to go\n",
    "    \n",
    "### Loading a remote dataset\n",
    "\n",
    "- If you're working as a data scientist or coder in a company, there's a good chance the datasets you want to analyze are stored on some remote server\n",
    "- Loading remote files is as simple as loading local ones\n",
    "- instead of pointing 'data_files' to the local files 'path' we point instead to one or more 'URLs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c73697f3-725b-4ee7-a448-0b4fb166936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-57dcee3ea6992346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-57dcee3ea6992346/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbff5309998c4d5eb7b096f6ed0fa54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81230fb280554cdb9ddb3fda9438d542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585cdfaf430c472dbc3a378b91b76b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d680edb7de047548960b2172988acc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-57dcee3ea6992346/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97082b2660ba4dbd8ef74101e4344096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://github.com/crux82/squad-it/raw/master/\"\n",
    "data_files = {\n",
    "    'train': url + \"SQuAD_it-train.json.gz\",\n",
    "    'test': url + \"SQuAD_it-test.json.gz\",\n",
    "}\n",
    "\n",
    "squad_it_dataset = load_dataset(\"json\", data_files = data_files, field = \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b98e9ff1-da50-4fb7-9771-869cff9855e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wine.zip\n",
      "  inflating: Index                   \n",
      "  inflating: wine.data               \n",
      "  inflating: wine.names              \n"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "# Repeat what was taught but using your own dataset found online\n",
    "\n",
    "# get url (didn't know what was inside .zip file so didn't want to run load_dataset() on it)\n",
    "# !wget https://archive.ics.uci.edu/static/public/109/wine.zip # commented out\n",
    "\n",
    "# gzip is only for .gz files. since its a .zip file we'll use 'unzip'\n",
    "!unzip wine.zip\n",
    "\n",
    "# im coming back to this one, \"DONT USE THE WINE DATSET, its probably because its old '1991', had some shit about converting a .db file to a json file and something broke\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
