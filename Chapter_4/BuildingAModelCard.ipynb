{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf5d17d-0f06-44c1-9c6a-ee10136360f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0K\tBuildingAModelCard.ipynb\n",
      "1.5K\t.ipynb_checkpoints\n",
      "7.5K\tUsing_pretrained_models.ipynb\n",
      "23K\tSharing_pretrained_models.ipynb\n",
      "33K\ttotal\n"
     ]
    }
   ],
   "source": [
    "# check space used in paperspace\n",
    "!du -sch .[!.]* * | sort -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42c05e-654f-4ae3-9862-f84c4fffb401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"oh-yeontaek/llama-2-13B-LoRA-assemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701d4b82-c3c1-44de-9550-32d984639b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           44Gi       1.9Gi        27Gi       208Mi        14Gi        41Gi\n",
      "Swap:            0B          0B          0B\n"
     ]
    }
   ],
   "source": [
    "# check current ram usage\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4cc8b-2bad-4052-b585-fda3ad066461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script for loading model - final version\n",
    "import psutil\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Global flag to control the monitoring loop\n",
    "keep_monitoring = True\n",
    "\n",
    "# Define the function to monitor memory\n",
    "def monitor_memory(interval=1):\n",
    "    global keep_monitoring\n",
    "    print(\"Starting memory monitoring...\")\n",
    "    try:\n",
    "        while keep_monitoring:\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            print(f\"Used RAM: {memory_info.used / (1024**3):.2f} GB, Available RAM: {memory_info.available / (1024**3):.2f} GB\")\n",
    "            time.sleep(interval)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Memory monitoring stopped.\")\n",
    "\n",
    "# Start monitoring memory in a separate thread\n",
    "monitoring_thread = threading.Thread(target=monitor_memory, args=(1,))\n",
    "monitoring_thread.start()\n",
    "\n",
    "try:\n",
    "    # Now, run your code that might be causing the issue. The memory usage will be printed above.\n",
    "    # Load model directly\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"MindNetML/llama2-yeontaek\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"MindNetML/llama2-yeontaek\", device_map=\"auto\", torch_dtype=torch.float16)\n",
    "    \n",
    "    # If the code runs successfully, stop the monitoring\n",
    "    keep_monitoring = False\n",
    "    monitoring_thread.join(timeout=1)\n",
    "    print(\"Memory monitoring stopped after successful execution.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # If there's an error, you can decide whether to stop the monitoring or not.\n",
    "    # Uncomment the next two lines if you want to stop monitoring on error.\n",
    "    keep_monitoring = False\n",
    "    monitoring_thread.join(timeout=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29ed00-9aaf-4a9b-8da2-4cb852424058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to use 'cuda'\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed4f2db-2f8c-451d-8f9e-dc5b316d56a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pacer gram pacer test is a test that measures the heart's ability to pump blood during exercise. It involves walking on a treadmill while the heart rate is monitored. The test helps doctors determine if the heart is receiving enough oxygen during exercise and if there are any underlying heart problems.\n",
      "\n",
      "The test is performed as follows:\n",
      "\n",
      "1. Preparation: Before the test, the patient should wear comfortable clothing and shoes. They should also inform the doctor about any medications they are taking, as some may need to be adjusted or stopped before the test.\n",
      "\n",
      "2. Attaching electrodes: The patient will be asked to lie down on a table or bed. Electrodes will be attached to their chest, arms, and legs to monitor the heart's electrical activity during the test.\n",
      "\n",
      "3. Starting the treadmill: The patient will be asked to walk on a treadmill, starting at a slow pace and gradually increasing the speed and incline.\n",
      "\n",
      "4. Monitoring heart rate: Throughout the test, the patient's heart rate will be monitored using an electrocardiogram (ECG) or a heart rate monitor.\n",
      "\n",
      "5. Stopping the test: The test will be stopped if the patient experiences chest pain, shortness of breath, dizziness, or any other symptoms that indicate a problem.\n",
      "\n",
      "6. Recovery: After the test, the patient will be asked to rest for a few minutes before being helped off the treadmill.\n",
      "\n",
      "7. Results interpretation: The doctor will analyze the heart rate and other data collected during the test to determine if the heart is functioning properly during exercise.\n",
      "\n",
      "The pacer gram pacer test is a non-invasive way to assess the heart's performance during exercise and can help diagnose various heart conditions, such as coronary artery disease, heart valve problems, or irregular heart rhythms.\n"
     ]
    }
   ],
   "source": [
    "# Define your prompt\n",
    "prompt = \"The pacer gram pacer test\"\n",
    "\n",
    "# Encode your prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# set input_ids to use 'cuda'\n",
    "input_ids = input_ids.to('cuda')\n",
    "\n",
    "# Generate text from your prompt\n",
    "output_ids = model.generate(input_ids, max_length = 1024)\n",
    "\n",
    "# Decode the output\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the output\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7046353b-eb0d-43d2-aef0-d18749a2f5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cde104e4b64478c8e6db4439019072b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c902af8f6ee84185a437d43a56c30950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecba498ef4ae49928ee2399d0723c961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2339b705f0a3456f88fd0bb6a410272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MindNetML/llama2-yeontaek/commit/3de2ca66f93c6eeb0bcc441f29857908abe31382', commit_message='Upload tokenizer', commit_description='', oid='3de2ca66f93c6eeb0bcc441f29857908abe31382', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
