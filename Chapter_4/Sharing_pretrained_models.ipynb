{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1006ed4c-1f8f-41e6-80b6-751460b296aa",
   "metadata": {},
   "source": [
    "### There are three ways to go about creating new model repositories:\n",
    "- Using the 'push_to_hub' API\n",
    "- Using the 'huggingface_hub' python library\n",
    "- Using the web interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9219d155-0ea4-4887-9613-d7c338965c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb238ecf96a4245935807c66ae64637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The simplest way to upload files to the hub is by leveraging the 'push_to_hub' API\n",
    "# Notebook login\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3d3e01-2374-4798-8dc4-45221d745511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Terminal login\n",
    "!huggingface-cli login --token \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb0308-42f4-4e66-99bd-c8b5b1d6eb27",
   "metadata": {},
   "source": [
    "### The easiest way to upload to the Hub is to set 'push_to_hub = True' when you define your 'TrainingArguments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5016eb-3852-4f21-8e60-ed3a2117451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"bert-finetuned-mrpc\", save_strategy = 'epoch', push_to_hub = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e9e23-a8d8-4879-957b-23aab4c1cd85",
   "metadata": {},
   "source": [
    "- When you call 'trainer.train()' the 'Trainer' will then upload your model to the Hub each time it is saved (here every epoch)\n",
    "- That repository will be named like the ouput directory you picked (here bert-finetuned-mrpc) but you can choose a different name with 'hub_model_id = \"a_different_name\"'\n",
    "- To upload your model to an organization you are a member of, you pass it with 'hub_model_id = \"my_organization/my_repo_name\"'\n",
    "\n",
    "- Once your training is finished, you should do a final 'trainer.pysh_to_hub()' to upload the last version of your model.\n",
    "    - It will also generate a model card with all the relevant metadata, reporting the hyperparameters used and the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e8ed70-bf9b-44a8-bc88-e9b6dcd7a328",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/model_card.png\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "  \n",
    "# get the image\n",
    "Image(url=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/model_card.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4f3b1-0502-434a-a23d-a93237918902",
   "metadata": {},
   "source": [
    "- At a lower level the model Hub can be done directly on models, tokenizers and configuration objects via their 'push_to_hub()' method.\n",
    "    - This method takes care of both the repository creation and pushing the model and the tokenizers files directly to the repository\n",
    "    - No manual handeling is required, unlike the API we'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556ee6b1-eb2f-4051-8794-84fbbf5129de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "checkpoint = 'camembert-base'\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer  =AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d0c5de-5f65-4f0d-9b7e-93999cc2a456",
   "metadata": {},
   "source": [
    "#### Your free to do whatever you want with these *add tokens to the tokenizer, train the model, fine-tune it*\n",
    "- Once your happy with the resulting model, weights, and tokenizer, you can leverage the 'push_to_hub()' method directly available on the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a72af8-e9a1-4a7c-9eb9-17e6da63d2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214c3a1092c54f428b8c2c4e6aaa05fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MindNetML/dummy-model/commit/e1e47f493e3f4836f8165a235486c06a23375eb0', commit_message='Upload CamembertForMaskedLM', commit_description='', oid='e1e47f493e3f4836f8165a235486c06a23375eb0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('dummy-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df49452-ec83-42bb-8198-34af89e3db87",
   "metadata": {},
   "source": [
    "- The above will create a new repository 'dummy-model' in you profile, and populate it with your model files.\n",
    "- Do the same with the tokenizer, so that all the fils are now available in this repostory\n",
    "- If you belong to an organization, simply specify the organizatoin argument to upload to the organization's namespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f8dc9d-f451-4bd2-992f-e823109f0f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e7cd1396141929186bb2c2185bb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MindNetML/dummy-model/commit/5950520263d482ee6a982cc643673b59fc4e13eb', commit_message='Upload tokenizer', commit_description='', oid='5950520263d482ee6a982cc643673b59fc4e13eb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to hub while naming organization you belong to\n",
    "tokenizer.push_to_hub('dummy-model')\n",
    "\n",
    "# If you wish to specify a specific HuggingFace token\n",
    "# tokenizer.push_to_hub(\"dummy-model\", organization = \"huggingface\", use_auth_token = \"<TOKEN>\")\n",
    "\n",
    "# Don't run or you'll get ttwo more dummy-models pushed to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411baa63-5c67-4331-9b60-a6958c6c001b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "\n",
    "# name of model we'll use\n",
    "checkpoint = 'bert-base-cased'\n",
    "\n",
    "# download model and tokenizer for the checkpoint\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066d9533-8eb1-429c-9ca1-99784ad6f2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a335e702344639b1f6584ee0d4eade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MindNetML/bertDummy-model/commit/df51935a418bf04496d07dc9e69b37100bf0d64f', commit_message='Upload tokenizer', commit_description='', oid='df51935a418bf04496d07dc9e69b37100bf0d64f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('bertDummy-model')\n",
    "tokenizer.push_to_hub('bertDummy-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198b2fa-105e-4e77-b7bb-8ceeaf712152",
   "metadata": {},
   "source": [
    "### Using the huggingface_hub Python library\n",
    "- The 'huggingface_hub' python library offers tools for the model and dataset hubs.\n",
    "- it provides methods and classes for common task like getting information about repotories on the hub and managing them\n",
    "- It provides simple API's that work on top of git to manage those repositories content and to integrate the Hub in your projects and libraries\n",
    "- The 'push_to_hub' API will require you to have your API token saved in your cache.\n",
    "    - login using the CLI (hugging-cli login --token 'token')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36970984-eb22-4d66-bb2f-35ddcf49371b",
   "metadata": {},
   "source": [
    "- The hugginface_hub offers methods and classes which are useful for our purpose\n",
    "- Firstly there are a few mthods to manage repoitory creation, deletion, and other:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import (\n",
    "    # User management\n",
    "    login,\n",
    "    logout,\n",
    "    whoami,\n",
    "\n",
    "    # Repository creation and management\n",
    "    create_repo,\n",
    "    delete_repo,\n",
    "    update_repo_visibility,\n",
    "\n",
    "    # And some methods to retrieve/change information about the content\n",
    "    list_models,\n",
    "    list_datasets,\n",
    "    list_metrics,\n",
    "    list_repo_files,\n",
    "    upload_file,\n",
    "    delete_file,\n",
    ")\n",
    "```\n",
    "\n",
    "- Additionally, it offers the very powerful 'Repository' class to manage a local repostory. We will explore these methods and that class in the next few section to understand how to leverage them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0928fc21-21f2-41a4-8f72-d358829922b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/MindNetML/dummy-model2', endpoint='https://huggingface.co', repo_type='model', repo_id='MindNetML/dummy-model2')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 'create_repo' method creats a new repository in the hub\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "create_repo(\"dummy-model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d2fc7-30a7-4b68-b1f7-caccf73a34f3",
   "metadata": {},
   "source": [
    "- If you choose organization, the model will be featured on the organization's page and every memeber of the organizaition will have the ability to contribute to the rposiitory\n",
    "\n",
    "- Next, enter your models name, this will also be the name of the repository, finally, \n",
    "- You can specify whether you want your model to be public or private. \n",
    "- private models are hidden from public view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd452473-60f6-4d65-adb5-f26a9c371d8c",
   "metadata": {},
   "source": [
    "- The newly created repo wil be blank\n",
    "- This is where your model will be hoseted, to start populating it, you can add a README file directly fromt the web interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4469645-b055-40e5-b271-04f9b84cfd39",
   "metadata": {},
   "source": [
    "### The upload_file approach\n",
    "- using 'upload_file' does not require 'git' and 'git-lfs' to be installed on your system.\n",
    "- It pushes files directly to the ðŸ¤— Hub using HTTP POST requests.\n",
    "- A limitation of this approach is that it doesn't handle files that are larger than 5GB in size\n",
    "- If your files are larger than 5GB, please follow the two other methods detailed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fac1c6b7-8d38-42f2-ba79-7c944640c2a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "upload_file() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# The API may be used as follows\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upload_file\n\u001b[0;32m----> 4\u001b[0m \u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<path_to_file>/config.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMindNetML/dummy-model2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/huggingface_hub/hf_api.py:849\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: upload_file() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given"
     ]
    }
   ],
   "source": [
    "# The API may be used as follows\n",
    "# not working. Unsure why, to lazy to fix\n",
    "# skip it, not important\n",
    "from huggingface_hub import upload_file\n",
    "\n",
    "upload_file(\n",
    "    \"<path_to_file>/config.json\",\n",
    "    path_in_repo = \"config.json\",\n",
    "    repo_id = \"MindNetML/dummy-model2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d26850f-4237-4624-8e1c-8de5675d9a17",
   "metadata": {},
   "source": [
    "- The above will upload the file 'config.json' available at <path_to_file> to the root of the repository as config.json, to the dummy-model repostory.\n",
    "- Other arguments which may be useful are\n",
    "    - 'token', if you would like to override the token stored in your cache by a given token\n",
    "    - 'repo_type', if you would like to upload to a dataset or a space instead of a model. Accepted values are \"dataset\" and \"space\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7f311-26fa-4d19-aa28-5278531274f9",
   "metadata": {},
   "source": [
    "### The Repository Class\n",
    "\n",
    "- The Repository class manages a local repository in a git-like manner.\n",
    "- It abstracts most of the pain points one may have with git to provide all features that we require.\n",
    "- Using this class requires having git and git-lfs installed, so make sure you have git-lfs installed (see here for installation instructions) and set up before you begin\n",
    "\n",
    "- In order to start playing around with the repository we have just created, we can start by initlising it into a local folder by cloning the remote repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb7ede9-d316-4619-898f-974dae88acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/MindNetML/dummy-model2 into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(\"<path_to_dummy_folder>\", clone_from = \"MindNetML/dummy-model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516188a0-b5ce-4b0f-9f0b-a83314f94880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make sure that our local clone is up to dat by pulling the latest change\n",
    "repo.git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "361a7b76-6c5e-4712-898e-8ab2f854d4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<path_to_dummy_folder>/tokenizer_config.json',\n",
       " '<path_to_dummy_folder>/special_tokens_map.json',\n",
       " '<path_to_dummy_folder>/vocab.txt',\n",
       " '<path_to_dummy_folder>/added_tokens.json',\n",
       " '<path_to_dummy_folder>/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We save the model and tokenizer files\n",
    "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
    "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28162b23-e9df-44e0-baa1-6728a88a8f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8229161da25a48c5853eda2926403835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/MindNetML/dummy-model2\n",
      "   f13c2fe..e8b8ebd  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/MindNetML/dummy-model2/commit/e8b8ebdac3a0cc8576e07a866ebe84c542ad36bb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the <path_to_dummy_folder> contains all the model and tokenizer files. \n",
    "\n",
    "# we'll follow the usual git workflow by adding files to the staging area, commiting them and pushing them to the hub:\n",
    "repo.git_add()\n",
    "repo.git_commit(\"Add model and tokenizer files\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acb721-2dcb-4cc7-aef4-825bec385ade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
