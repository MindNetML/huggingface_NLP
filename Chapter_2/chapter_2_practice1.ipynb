{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30456665-31c1-4e0d-bda7-72ea9fae50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Autotokenizer for converting text to tokens\n",
    "# import AutoModelForSequenceClassification to download head of sequence classification (seeing if a sentence is positve or negative)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# load model name we will use into checkpoint variable\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# load Tokenizer and AutoModelForSequenceClassification and use the above model and save it to the 'tokenizer' & 'model' variables\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f12b067-ed48-4211-8fc6-8fdf1611707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The raw input we will load into the model\n",
    "raw_inputs = [\n",
    "    \"Potato wedges probably are not best for relationships.\",\n",
    "    \"No matter how beautiful the sunset, it saddened her knowing she was one day older.\",\n",
    "    \"Everything was going so well until I was accosted by a purple giraffe.\",\n",
    "    \"They say people remember important moments in their life well, yet no one even remembers their own birth.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e6677e0-9547-4cb8-8044-bf804f768048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 14557, 17632,  2015,  2763,  2024,  2025,  2190,  2005,  6550,\n",
       "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0],\n",
       "        [  101,  2053,  3043,  2129,  3376,  1996, 10434,  1010,  2009,  6517,\n",
       "         24589,  2014,  4209,  2016,  2001,  2028,  2154,  3080,  1012,   102,\n",
       "             0,     0],\n",
       "        [  101,  2673,  2001,  2183,  2061,  2092,  2127,  1045,  2001, 16222,\n",
       "         14122,  2098,  2011,  1037,  6379, 21025, 27528,  7959,  1012,   102,\n",
       "             0,     0],\n",
       "        [  101,  2027,  2360,  2111,  3342,  2590,  5312,  1999,  2037,  2166,\n",
       "          2092,  1010,  2664,  2053,  2028,  2130, 17749,  2037,  2219,  4182,\n",
       "          1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we pass the inputs into the tokenizer so the text will be in a format our model can read\n",
    "\n",
    "# we'll save it in the 'inputs' variable\n",
    "inputs = tokenizer(raw_inputs, padding=True, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "893556bd-0a2a-4fdd-9305-7d8462e4cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.3587, -3.5323],\n",
      "        [ 0.9353, -0.7998],\n",
      "        [-0.7023,  0.8136],\n",
      "        [ 3.2423, -2.6515]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we can check the outputs the model will give by passing the above output the tokenizer gave us into the model\n",
    "\n",
    "# we basically pass what was printed above into the model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# We pass logits as that is what our model outputs without the softmax layer.\n",
    "# Bascially this isn't the final output\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "875e6895-94f2-4262-be21-c52601324c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9963e-01, 3.7393e-04],\n",
       "        [8.5006e-01, 1.4994e-01],\n",
       "        [1.8006e-01, 8.1994e-01],\n",
       "        [9.9725e-01, 2.7489e-03]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the final output\n",
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69d909f6-38c7-4ed3-9fcc-1faa6a04f48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9996260404586792},\n",
       " {'label': 'NEGATIVE', 'score': 0.8500564694404602},\n",
       " {'label': 'POSITIVE', 'score': 0.8199344277381897},\n",
       " {'label': 'NEGATIVE', 'score': 0.9972510933876038}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the above steps can be achieved with the pipeline class\n",
    "from transformers import pipeline\n",
    "\n",
    "easy_model = pipeline(\"sentiment-analysis\", model = checkpoint)\n",
    "\n",
    "easy_model(raw_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
