{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa9a21a-cf4c-4600-90d6-0a6b97d4e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers torch torchvision torchaudio\n",
    "!pip install -q tokenizers==0.13.3\n",
    "!pip install -q bitsandbytes transformers accelerate gradio thread6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea01f1fe-ad87-4289-9ce0-d00e78ba2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers can't process text directly. Language models first use a tokenizer to convert text inputs into numbers that the model can understand\n",
    "\n",
    "# Tokenizers are responsible for:\n",
    "    # Splitting the inputs into words, subwords, or symbols (like punctuations) that are called tokens\n",
    "    # Mapping each token to an integer\n",
    "    # Adding additional inputs that may be useful to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cfc3e8-8c35-4205-b182-6689cb7ea34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf13fb9-7500-44b1-93ce-e2db5f1b92cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6eabc811d1b49439108d374757569e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5147ccff1a294d8aa7b5490292681ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3977a26384234705aca840c09e09c66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We get a tokenizer from the distilbert-base-uncased-finetuned-sst-2-english for the example\n",
    "# Were going to see how the tokenizer works\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b60d4d8-722c-4338-bcab-123de701dd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# We get the text were going to pass through the tokenizer\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "# Next we call the tokenizer with the text along with arguments we'll touch on later\n",
    "# focus on the concept of what the tokenizer does to the above text\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, return_tensors='pt')\n",
    "print(out)\n",
    "\n",
    "# The output is a dictionary containing 2 keys, [input_ids, attention_mask]\n",
    "# The input id's are the numerical representation of the text above, the attention mask will be discussed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "546e2f38-65d2-44e8-8165-40acd6bb6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now download the model similarly to how we downloaded the tokenizer\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee66539-167a-4023-88ec-e2385642b8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdfa29-4fe1-407e-a8f7-de50ce12f248",
   "metadata": {},
   "source": [
    "### 1. Transformers \n",
    "When a model processes input data, it produces outputs known as \"hidden states\" or \"features\"\n",
    "\n",
    "### 2. Hidden States/Features:\n",
    "These Hidden States are essentially high-demensional vectors that capture the contextual understanding of the input by the Transformers model. Think of these vectors as the Transformers interpretation or representation of the input data.\n",
    "\n",
    "### 3. Heads:\n",
    "While hidden states are informative, they are not the end of the processing pipeline. Depending on the task at hand(text classification, text generation), different heads can be attached to the base model. Each task might require a different type of processing in its head, but the  underlying architecture(the base Transformer module) remains the same.\n",
    "\n",
    "### 4. High-dimensional vector:\n",
    "When the Transformer processes input data, the output is a vector with three dimensions:\n",
    "    * Batch size: Refers to how many sequences are processed simultaneously.\n",
    "    * Sequence length: Refers to the length of the input sequence's numerical representation\n",
    "    * Hidden size: Refers to the size or dimensionality of the hidden state (the vector) for each input. This can be quite large (e.g., 768 for smaller models, and even 3072 or       more for bigger ones). Hence, the term \"high-dimensional.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fae10d-8d54-41ef-bde1-60ff93a3450b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg\" width=\"900\" height=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The below image is how the model handles the hidden states and the head\n",
    "\n",
    "# import image module\n",
    "from IPython.display import Image\n",
    "  \n",
    "# get the image\n",
    "Image(url=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d964288-38aa-41ec-b4ed-85b0efc47ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Hidden States/Features\" \n",
    "outputs = model(**inputs)\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5463c-d586-480e-b584-d81ca5f43def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many different architectures available in ðŸ¤— Transformers, each designed around a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f7f453b-8641-4b7a-87c9-eb6abbe1f018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1f85db4-aa6e-4177-a26d-a601ffa0ba9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# We chose a different head for the same model we called a while ago\n",
    "# Notice that the output of this head changed the dimensionality of our output\n",
    "# The head takes as input high-dimensional vectors and outputs vectors containing two values. (one per label)\n",
    "# we get [2,2] since we used the 2 sentences as input earlier in this notebook\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afc15183-c9c3-43e9-8495-b1a9ee33db9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Postprocessing the output\n",
    "\n",
    "# The model outputed [-1.5607,  1.6123] for the first and [ 4.1692, -3.3464] for the second sentence\n",
    "# The correct term for them currenlty is \"Logits\"\n",
    "# The values alone make no sense but this is becuase the values are not yet normalized\n",
    "# They need to be passed through a \"Soft Max Layer\" to be converted to probabilities (The current head gets probabilities of sentences being positive or negative)\n",
    "\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57384976-31df-45e3-accd-97483cebba1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0195e-02, 9.5980e-01],\n",
       "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's pass them through a softmax layer\n",
    "# BOOM, we got our predictions\n",
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "765388a4-d524-4f30-a0dc-04c70d47db94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the correct label for the probabilities we can call the id2label attribute\n",
    "\n",
    "model.config.id2label\n",
    "\n",
    "# The first number is negative and the second is positive\n",
    "# First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
    "# Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5463c-2688-4660-b2f8-408ecbb1caeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
