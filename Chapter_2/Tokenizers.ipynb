{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ecac4c-105d-409c-b0a0-2d9480d465bf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers torch torchvision torchaudio\n",
    "!pip install -q tokenizers==0.13.3\n",
    "!pip install -q bitsandbytes transformers accelerate gradio thread6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a182533-0b3c-492f-b598-bd4a53fbbcd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Theres many types of tokenizers\n",
    "##### The first type we'll touch on is word-based tokenization\n",
    "##### It's generally easy to setup, has few rules, and yeilds decent results\n",
    "##### Below, the goal is to split the raw text into words and find a numberical represntaion for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bcf35bd-d19f-41f5-a8ca-31c8c46c091f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Based Tokenization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg\" width=\"900\" height=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "print(\"Word Based Tokenization\")\n",
    "Image(url=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization-dark.svg\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f26898-00e5-47d4-93bc-e3e59a09443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jim', 'Henson', 'was', 'a', 'puppeteer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There's  different ways to split the text. The example above uses splits on spaces and on punctuations.\n",
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "\n",
    "# This example tokenizes on spaces\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abc2402-0033-4968-9121-8dd8c6e6b2b4",
   "metadata": {},
   "source": [
    "##### Both types of the above tokenizations are not recommended as they have a lot of draw backs such as large vocabuallry size, \n",
    "##### Also, words like dog and dogs are counted as not being similar\n",
    "##### A custom token is also added for unknown words, its typically \"[UNK]\"\n",
    "\n",
    "#### One way to reduce the amount of tokens is to use 'Character-based' tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc777ac1-f9dc-4b54-98da-07ed1ee7c613",
   "metadata": {},
   "source": [
    "### Character based tokenization split the text into characters(letters, numbers, symbols), rather than words. \n",
    "\n",
    "#### The provides 2 benefits\n",
    "  ##### - The vocabulary is much smaller\n",
    "  ##### - There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters\n",
    "\n",
    "#### Another thing to consider is that we'll end up with a avery large amount of tokens to be processed by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d94cead7-c228-4f12-897e-615bb1dd8424",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character based tokenization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg\" width=\"900\" height=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "print(\"Character based tokenization\")\n",
    "Image(url=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization-dark.svg\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142de5ad-cbcf-47f4-b5e7-3f8a615136c8",
   "metadata": {},
   "source": [
    "### Sub-word Tokenization\n",
    "\n",
    "#### Relie's on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed inoto meaningful subwords\n",
    "\n",
    "   ##### - For example: \"Annoyingly\" can be considered rare since its composed of both \"Annoying\" and \"ly\". Theses are likely to appear often as standalone subwords, while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa381a5-2695-4178-abea-fa65593a8fed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subword tokenization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg\" width=\"900\" height=\"900\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "print(\"subword tokenization\")\n",
    "Image(url=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword-dark.svg\", width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1ca09-492d-456a-ba5a-158a6459f509",
   "metadata": {},
   "source": [
    "##### Loading and saving tokenizers is the same as with model using the from_pretrained() and the save_pretrained() methods\n",
    "##### These methods will save the algorithm used by the tokenizer as well as the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9b82348-2ce0-4dec-b3c5-12d2a34deb5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efe996e56d049ae926f3a14c35d23bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76159ce83904d83ad11c2736bc2fe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the BERT tokenizer trained with the same checkpoint as BERT\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4bb5fee-cd9a-4195-b603-d6f53ec564c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e42fa724f74782ac866671ec6c5d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name\n",
    "# it can be used directly with any checkpoint\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6eac7e8-c784-4fe9-a073-7d899ef71d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1124, 1125, 170, 14673, 2305, 1115, 2863, 1522, 3485, 1106, 23570, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets use the tokenizer\n",
    "tokenizer(\"He had a vague sense that trees gave birth to dinosaurs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74068b35-d49f-4790-bd8b-73610b3d8328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/notebooks/NLP_huggingface/Chapter_2/tokenizer_config.json',\n",
       " '/notebooks/NLP_huggingface/Chapter_2/special_tokens_map.json',\n",
       " '/notebooks/NLP_huggingface/Chapter_2/vocab.txt',\n",
       " '/notebooks/NLP_huggingface/Chapter_2/added_tokens.json',\n",
       " '/notebooks/NLP_huggingface/Chapter_2/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving a tokenizer is the same as saving a model\n",
    "tokenizer.save_pretrained(\"/notebooks/NLP_huggingface/Chapter_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07b5e2f-b7c9-4ac7-967d-230906cc61c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b35e991c754054abd9968953c03c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c6b6488b624e78964ddcf983e1cdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d856c5d17bb4a469c2ad5fa752cac60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d4d8d0034f4b1382df4b448a02c7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're going to tokenize some input and print its tokenized form\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# The outuput is a list of strings, Notice how its using sub-word tokenization\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06dda458-baf4-4068-a3ae-431e33d427bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7993, 170, 13809, 23763, 2443, 1110, 3014]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The conversion to input ID's is handled by the convert_tokens_to_ids() tokenizer method\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5566b4ac-0fe3-47e7-b8f8-288d12e4b92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '’', 've', 'been', 'waiting', 'for', 'a', 'Hu', '##gging', '##F', '##ace', 'course', 'my', 'whole', 'life', '.', 'I', 'hate', 'this', 'so', 'much', '!']\n",
      "[146, 787, 1396, 1151, 2613, 1111, 170, 20164, 10932, 2271, 7954, 1736, 1139, 2006, 1297, 119, 146, 4819, 1142, 1177, 1277, 106]\n"
     ]
    }
   ],
   "source": [
    "# Exercise example\n",
    "\n",
    "# sample text to tokenize\n",
    "example_sequence = [\n",
    "    \"I’ve been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "\n",
    "# tokenize sample text and store it in variable\n",
    "example_tokens = tokenizer.tokenize(example_sequence)\n",
    "\n",
    "print(example_tokens)\n",
    "\n",
    "# convert tokenized text to input ID's\n",
    "example_ids = tokenizer.convert_tokens_to_ids(example_tokens)\n",
    "\n",
    "print(example_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb77f542-0cac-47b9-80e4-b9668e694ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I ’ ve been waiting for a HuggingFace course my whole life. I hate this so much!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoding means going backwards, so input ID's -> tokenized text -> text\n",
    "decoded_string = tokenizer.decode(example_ids)\n",
    "\n",
    "# Note that the decode method not only converts back tokens, but also groups together the tokens that were part of the same words to produce a readable sentence.\n",
    "decoded_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
